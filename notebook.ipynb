{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qevAEi3GXYuu"
      },
      "source": [
        "# Amazon reviews K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw5omSuCXYuw"
      },
      "source": [
        "1.   Scaricare i dati disponibili a questo url: [amazon reviews](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download) o questo [amazon review dropbox](https://www.dropbox.com/scl/fi/ucfoh391qalha3lz0bzjx/amazon_review_polarity_csv.tgz.zip?rlkey=m3a0bbp2ep4sh2qisaz0xwo1w&dl=0)\n",
        "\n",
        "\n",
        "2.   Il dataset è composto da due file: train and test in csv. Ogni file contiene le seguenti informazioni\n",
        "  *   polarity - 1 for negative and 2 for positive\n",
        "  *   title - review heading\n",
        "  *   text - review body\n",
        "\n",
        "3.  Generare i vettori sparsi applicando il q-shingle ai dati di training con q=3.\n",
        "4. Sui vettori sparsi Applicare il MinHashing LSH sul dataset di training.\n",
        "5. USare il file di testing e applicare una k-nearest neighbor con i dati di testing su cui è stato applicato l'hashing. Usare k=3 e classificare l'elemento con del test set con la polarità maggiormente presente.\n",
        "\n",
        "\n",
        "6. *Identificare i cluster di recensioni. Ogni cluster di recensione contiene le coppie di recensioni che hanno una similarità  > di 0.6. Da svolgere dopo l'introduzione alle network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2EAxqknBXYux"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.mllib import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.feature import MinHashLSH\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from collections import Counter\n",
        "from pyspark.sql.functions import lit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AJpmgE5SXYuy"
      },
      "outputs": [],
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKAWuwBXYuz"
      },
      "source": [
        "# Getting the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jL3ljQfXYuz",
        "outputId": "de4d94e5-b690-4f32-8525-567a4e426bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- polarity: integer (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField('polarity', IntegerType(), True),\n",
        "    StructField('title', StringType(), True),\n",
        "    StructField('description', StringType(), True),\n",
        "])\n",
        "df_train = spark.read.format(\"csv\") \\\n",
        "               .option(\"header\", \"false\") \\\n",
        "               .schema(schema) \\\n",
        "               .load(\"train.csv\")\n",
        "\n",
        "# Mostra lo schema del DataFrame\n",
        "df_train.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY2Z7foDXYuz",
        "outputId": "2b49002a-3d69-4af0-e2cf-d566f3e20c68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25755"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_train.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCcGUmwNXYu0",
        "outputId": "72015b9b-526c-41c0-82bf-519ed2632cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- polarity: integer (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_test = spark.read.format(\"csv\") \\\n",
        "               .option(\"header\", \"false\") \\\n",
        "               .schema(schema) \\\n",
        "               .load(\"test.csv\")\n",
        "\n",
        "# Mostra lo schema del DataFrame\n",
        "df_test.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34U4lqstXYu0",
        "outputId": "bdbedfda-e56d-46da-ab90-374d0ff884c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160249"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZLWtJtwXYu1"
      },
      "source": [
        "# Q-shingle\n",
        "$q = 3$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIYJfRRsXYu1"
      },
      "outputs": [],
      "source": [
        "def shingle(text: str, q: int):\n",
        "    shingle_set = []\n",
        "    if(text is None):\n",
        "        return list()\n",
        "\n",
        "    for i in range(len(text) - q + 1):\n",
        "        shingle_set.append(text[i:i+q])\n",
        "    return list(set(shingle_set))\n",
        "\n",
        "shingle_udf = F.udf(shingle, ArrayType(StringType()))\n",
        "\n",
        "q = 3\n",
        "df_train = df_train.limit(10).withColumn(\"shingles\", shingle_udf(F.col(\"description\"), F.lit(q)))\n",
        "\n",
        "# Mostra il DataFrame con i shingles\n",
        "df_train.select('shingles').show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bId4Q6lSXYu2"
      },
      "outputs": [],
      "source": [
        "shingles_df = df_train.select(F.explode(F.col('shingles')).alias('shingle')).distinct().orderBy(\"shingle\")\n",
        "shingles_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68OIsHLWXYu2"
      },
      "outputs": [],
      "source": [
        "shingles_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJPugT-KXYu2"
      },
      "outputs": [],
      "source": [
        "# Creazione del CountVectorizer con binary=True per ottenere un vettore binario (multi-hot encoding)\n",
        "cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"one_hot_shingles\", binary=True)\n",
        "\n",
        "# Fit del modello sul dataset e trasformazione\n",
        "cv_model = cv.fit(df_train)\n",
        "df_train = cv_model.transform(df_train)\n",
        "\n",
        "# Visualizzare il risultato\n",
        "df_train.select(\"one_hot_shingles\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP6s6D0wXYu2"
      },
      "source": [
        "# Min-hash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAei42siXYu2"
      },
      "outputs": [],
      "source": [
        "mh = MinHashLSH(inputCol=\"one_hot_shingles\", outputCol=\"hashes\", numHashTables=3)\n",
        "model = mh.fit(df_train)\n",
        "\n",
        "df_train = model.transform(df_train)\n",
        "df_train.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PaStPXrXYu3"
      },
      "source": [
        "# K-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWJDzbnxXYu3"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.withColumn(\"shingles\", shingle_udf(F.col(\"description\"), F.lit(q)))\n",
        "df_test.select('shingles').show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60eDw7nlXYu3"
      },
      "outputs": [],
      "source": [
        "# Mostra il DataFrame con i shingles\n",
        "df_test = cv_model.transform(df_test)\n",
        "df_test.select('one_hot_shingles').show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NjnDEchXYu3"
      },
      "outputs": [],
      "source": [
        "df_test.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQZFvFVeXYu3"
      },
      "outputs": [],
      "source": [
        "rows = df_train.limit(1000).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7MjVH7FXYu3"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "\n",
        "predictions = []\n",
        "for row in rows:\n",
        "\tneighbors = model.approxNearestNeighbors(df_train, row[\"one_hot_shingles\"], k)\n",
        "\n",
        "\tresult_row = neighbors.select(round(avg(col(\"polarity\"))).alias('pred_polarity')).first()\n",
        "\n",
        "\tif result_row and result_row[0] is not None:\n",
        "\t\t\tpred_polarity = int(result_row[0])\n",
        "\telse:\n",
        "\t\t\tif not result_row:\n",
        "\t\t\t\t\t\tprint(f\"Warning: No neighbors found for row. Using default polarity (None).\")\n",
        "\t\t\telse:\n",
        "\t\t\t\t\t\tprint(f\"Warning: Average polarity calculation resulted in None for row. Using default polarity (None).\")\n",
        "\t\t\tpred_polarity = None\n",
        "\n",
        "\tpredictions.append((row[\"title\"], row[\"description\"], row[\"polarity\"], pred_polarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_N0msGNXYu3"
      },
      "outputs": [],
      "source": [
        "df_predictions = spark.createDataFrame(predictions, [\"title\", \"description\", \"polarity\", \"pred_polarity\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ1Bv2qnXYu3"
      },
      "outputs": [],
      "source": [
        "df_predictions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wldrxgDgXYu3"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM4OdgqkXYu3"
      },
      "outputs": [],
      "source": [
        "errors = df_predictions.filter(col(\"polarity\") != col(\"pred_polarity\")).count()\n",
        "total = df_predictions.count()\n",
        "error_rate = errors / total\n",
        "\n",
        "print(f\"Error Rate: {error_rate:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}